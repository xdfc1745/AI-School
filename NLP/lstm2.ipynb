{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"g2C9h5qC9qDv","executionInfo":{"status":"ok","timestamp":1612946248115,"user_tz":-540,"elapsed":2661,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Embedding\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ElWcBGG0FGxJ"},"source":["# 간단하게 처리하는 방법\n","# from tensorflow.keras.datasets import imdb\n","\n","# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhPk94yGhcKZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612946360052,"user_tz":-540,"elapsed":6738,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}},"outputId":"81eb756e-89a3-4c3f-8a81-4ac635126aea"},"source":["from tensorflow.keras.datasets import imdb\r\n","(X_train, y_train), (X_test, y_test) = imdb.load_data()\r\n","X_train"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n","       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n","       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n","       ...,\n","       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n","       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n","       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"4b8Urt7aEfQJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612946475158,"user_tz":-540,"elapsed":24665,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}},"outputId":"c67abeae-ca2a-4b03-b7c9-3e860367ad36"},"source":["import pandas as pd\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","\n","\n","imdb_data = pd.read_csv(\"/content/drive/My Drive/NLP/data/IMDB Dataset.csv\")\n","# pos, neg 값을 숫자로 변환\n","imdb_data['sentiment'] = imdb_data['sentiment'].replace('positive', 1)\n","# imdb_data['sentiment'].replace('positive', 1, inplace=True)\n","imdb_data['sentiment'] = imdb_data['sentiment'].replace('negative', 0)\n","# 정규표현식을 써서, 단어가 아니면 공백으로\n","imdb_data['review'] = imdb_data['review'].str.replace(\"[^\\w]|br\", \" \")\n","# 혹시 공백이 있으면 제거\n","imdb_data['review'] = imdb_data['review'].replace(\"\", np.nan)\n","imdb_data['sentiment'] = imdb_data['sentiment'].replace(\"\", np.nan)\n","# null array 없애는 함수\n","imdb_data = imdb_data.dropna(how='any')\n","\n","print(\"# preprocessing done\")\n","\n","review_train, review_test, y_train, y_test = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size=0.25, shuffle=False, random_state=23)\n","\n","print(\"# split done\")\n","\n","stopwords = ['a', 'an']\n","\n","X_train = []\n","for stc in review_train:\n","    token = []\n","    words = stc.split()\n","    for word in words:\n","        if word not in stopwords:\n","            token.append(word)\n","    X_train.append(token)\n","\n","X_test = []\n","for stc in review_test:\n","    token = []\n","    words = stc.split()\n","    for word in words:\n","        if word not in stopwords:\n","            token.append(word)\n","    X_test.append(token)\n","\n","print(\"# tokenization done\")\n","\n","# 왜 트레인셋만?.. 원하신다면 처음에 test/train 스플릿하기 전에, 전처리하셔서 fit하셔도\n","# 왜 5000?.. 유의미한 단어 갯수를 생각해보자! 빈도수 1개다 -> 버리자, 빈도수 2개다 -> 버리자\n","tokenizer = Tokenizer(5000)\n","# train 을 기준으로 단어마다의 인덱스를 부여\n","tokenizer.fit_on_texts(X_train)\n","\n","# 부여된 정수 인덱스로 변환\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","\n","print(\"# int_encoding done\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["# preprocessing done\n","# split done\n","# tokenization done\n","# int_encoding done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fOoCjeLpV59q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612946386581,"user_tz":-540,"elapsed":17970,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}},"outputId":"a29412e7-a240-4f99-97ef-d082ff628f7d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pb7jiQeYX3lt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612946567949,"user_tz":-540,"elapsed":786,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}},"outputId":"b41a1e78-03a8-48e1-c4e6-2fb642e99b01"},"source":["print(X_train[1])\n","print(y_train[1])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[106, 387, 118, 357, 1, 1364, 3016, 5, 51, 51, 158, 54, 2198, 1550, 2, 409, 2, 527, 277, 3, 1847, 4, 1, 444, 408, 1, 150, 24, 566, 67, 2206, 494, 4098, 22, 60, 44, 192, 29, 1, 17, 23, 44, 29, 1, 2316, 176, 3408, 95, 19, 48, 371, 61, 1, 795, 32, 1, 1848, 4, 1798, 22, 60, 5, 6, 67, 273, 1, 147, 17, 6, 5, 412, 2, 2441, 408, 106, 4391, 357, 42, 27, 3, 1, 79, 1146, 11, 3, 201, 2, 25, 113, 1, 1847, 63, 267, 348, 15, 1, 118, 179, 1, 1030, 3, 1, 2791, 59, 249, 72, 356, 1, 2210, 960, 3198, 1249, 1202, 89, 4982, 6, 304, 21, 256, 1864, 2, 256, 4320, 572, 15, 1, 134, 3643, 2, 2, 1, 712, 572, 3, 64, 1047, 15, 11, 171, 2348, 24, 2010, 67, 218]\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dd6GpxL3FiuH","executionInfo":{"status":"ok","timestamp":1612946571228,"user_tz":-540,"elapsed":1881,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}}},"source":["# 문장 길이를 맞춰준다\n","# 임의로 맞추는게 아니고, 데이터셋을 보면서 최대 문장의 길이가 얼마인지 확인하시고 거기에 맞춰서\n","# imdb일때는 500, 네이버일때는 50으로 맞췄었습니다!\n","max_len = 500\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"cvJuOr7aGXI2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612946577226,"user_tz":-540,"elapsed":895,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}},"outputId":"46f3752b-6bce-4e9a-c33e-769745fa5256"},"source":["print(X_train[1])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0  106  387  118  357    1 1364 3016    5\n","   51   51  158   54 2198 1550    2  409    2  527  277    3 1847    4\n","    1  444  408    1  150   24  566   67 2206  494 4098   22   60   44\n","  192   29    1   17   23   44   29    1 2316  176 3408   95   19   48\n","  371   61    1  795   32    1 1848    4 1798   22   60    5    6   67\n","  273    1  147   17    6    5  412    2 2441  408  106 4391  357   42\n","   27    3    1   79 1146   11    3  201    2   25  113    1 1847   63\n","  267  348   15    1  118  179    1 1030    3    1 2791   59  249   72\n","  356    1 2210  960 3198 1249 1202   89 4982    6  304   21  256 1864\n","    2  256 4320  572   15    1  134 3643    2    2    1  712  572    3\n","   64 1047   15   11  171 2348   24 2010   67  218]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5BSOtN80GiTG","executionInfo":{"status":"ok","timestamp":1612946581048,"user_tz":-540,"elapsed":1658,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}}},"source":["# 레이어들을 쌓을 모델을 생성\n","model = Sequential()\n","# 단어를 임베딩하는데, 5000개 (imdb) 혹은 20000개 (네이버) 의 단어를 120차원으로 내보내겠다\n","# 1인자 = 내가 넣을 단어의 갯수 (총 인덱스의 갯수), 2인자 = 출력할 차원 (덴스 벡터의 차원), 3인자 = 문장의 길이\n","model.add(Embedding(5000, 120))\n","# RNN - simpleRNN / LSTM\n","model.add(LSTM(120))\n","# 긍정/부정을 판단하니까 이진 분류 -> sigmoid 함수 사용\n","model.add(Dense(1, activation='sigmoid'))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"P761nRyqHLUB","executionInfo":{"status":"ok","timestamp":1612946582701,"user_tz":-540,"elapsed":770,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}}},"source":["# 혹시 5회 이상 검증데이터 loss가 증가하면, 과적합될 수 있으므로 학습을 조기종료!\n","early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","# 훈련을 거듭하면서, 가장 검증데이터 정확도가 높았던 순간을 체크포인트로 저장\n","model_check = ModelCheckpoint('the_best.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"29WEfh8GH3UH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612950564641,"user_tz":-540,"elapsed":179883,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}},"outputId":"eee39fde-cca6-42fc-b959-526e29d88a75"},"source":["# 긍정/부정을 판단하니까 손실함수는 이진 교차 엔트로피, 최적화는 adam, 평가 기준은 acc (출력할때 뜬다)\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, callbacks=[early_stop, model_check])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","586/586 [==============================] - 567s 964ms/step - loss: 0.5190 - acc: 0.7391 - val_loss: 0.3098 - val_acc: 0.8714\n","\n","Epoch 00001: val_acc improved from -inf to 0.87144, saving model to the_best.h5\n","Epoch 2/10\n","586/586 [==============================] - 560s 956ms/step - loss: 0.2598 - acc: 0.8963 - val_loss: 0.2761 - val_acc: 0.8850\n","\n","Epoch 00002: val_acc improved from 0.87144 to 0.88504, saving model to the_best.h5\n","Epoch 3/10\n","586/586 [==============================] - 563s 961ms/step - loss: 0.2159 - acc: 0.9160 - val_loss: 0.2991 - val_acc: 0.8886\n","\n","Epoch 00003: val_acc improved from 0.88504 to 0.88856, saving model to the_best.h5\n","Epoch 4/10\n","586/586 [==============================] - 568s 970ms/step - loss: 0.1724 - acc: 0.9351 - val_loss: 0.2930 - val_acc: 0.8905\n","\n","Epoch 00004: val_acc improved from 0.88856 to 0.89048, saving model to the_best.h5\n","Epoch 5/10\n","586/586 [==============================] - 576s 983ms/step - loss: 0.1484 - acc: 0.9449 - val_loss: 0.3442 - val_acc: 0.8887\n","\n","Epoch 00005: val_acc did not improve from 0.89048\n","Epoch 6/10\n","586/586 [==============================] - 574s 980ms/step - loss: 0.1448 - acc: 0.9454 - val_loss: 0.3409 - val_acc: 0.8870\n","\n","Epoch 00006: val_acc did not improve from 0.89048\n","Epoch 7/10\n","586/586 [==============================] - 569s 971ms/step - loss: 0.2459 - acc: 0.8961 - val_loss: 0.3735 - val_acc: 0.8612\n","\n","Epoch 00007: val_acc did not improve from 0.89048\n","Epoch 00007: early stopping\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fe7855e37b8>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"Jx-KCvH9sLQz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612950909501,"user_tz":-540,"elapsed":44741,"user":{"displayName":"김가영","photoUrl":"","userId":"04644482288290784914"}},"outputId":"1832be36-991a-4841-af2a-3356fba0a0e0"},"source":["# 정확도 측정\n","# 출력하면 [loss, acc]\n","print(model.evaluate(X_test, y_test))\n","# X 값은 전처리, 토큰화, 정수인코딩이 된 상태의 문장이어야\n","# print(model.predict(X))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["391/391 [==============================] - 44s 113ms/step - loss: 0.3735 - acc: 0.8612\n","[0.37350350618362427, 0.8611999750137329]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2x6J_KHmnpdn"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bc1mPNHNb79u"},"source":["########## 한글 전처리부분입니다 ########\n","\n","import pandas as pd\n","import numpy as np\n","from konlpy.tag import Okt\n","from sklearn.model_selection import train_test_split\n","\n","naver_data = pd.read_table(\"./drive/My Drive/ratings.txt\", encoding='utf-8')\n","# 단어 아니면 삭제\n","naver_data['document'] = naver_data['document'].str.replace(\"[^\\w]\", \" \")\n","# 혹시 공백이 있으면 null array로\n","naver_data['document'] = naver_data['document'].replace('', np.nan)\n","naver_data['label'] = naver_data['label'].replace('', np.nan)\n","# null array 모두 제거 (공백인 열 모두 제거)\n","naver_data = naver_data.dropna(how='any')\n","\n","print(\"# preproecssing done\")\n","\n","# test/train 스플릿하고\n","review_train, review_test, y_train, y_test = train_test_split(naver_data['document'], naver_data['label'], test_size=0.25, shuffle=False, random_state=1004)\n","\n","print('# split done')\n","\n","# stopwords 지정\n","stopwords = ['를', '을', '이', '가', '으로', '로', '에', '에서']\n","\n","# 토큰화 진행\n","X_train = []\n","for stc in review_train:\n","    token = []\n","    words = Okt().morphs(stc, stem=True)\n","    for word in words:\n","        if word not in stopwords:\n","            token.append(word)\n","    X_train.append(token)\n","\n","X_test = []\n","for stc in review_test:\n","    token = []\n","    words = Okt().morphs(stc, stem=True)\n","    for word in words:\n","        if word not in stopwords:\n","            token.append(word)\n","    X_test.append(token)\n","\n","print('# tokenization done')\n","\n","# X_train 단어들을 토대로 정수 인덱스 설정, 전체 단어 갯수 설정\n","# 유의미한 단어? 빈도수가 1~2개인 단어 버려도 큰 영향을 끼치지 않을것 -> count함수써서 빈도수 낮은 것들을 버리고, 남은 단어의 갯수들\n","tokenizer = Tokenizer(20000)\n","tokenizer.fit_on_texts(X_train)\n","\n","# 설정된 정수 인덱스를 토대로 변환\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","\n","print('# int_encoding done')"],"execution_count":null,"outputs":[]}]}